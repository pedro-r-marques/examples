#
# This playbook expects AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY to be
# defined.
# The AIM user needs priviledges to manage VPCs and EC2 instances.
#
- hosts: localhost
  connection: local
  gather_facts: False
  tasks:
    - name: Create a VPC
      ec2_vpc:
        state: present
        region: "{{ aws_region }}"
        cidr_block: 192.168.0.0/16
        resource_tags: {"Name": "k8s-vpc-{{ job_id }}", "Cluster": "k8s-ansible-{{ job_id }}"}
        subnets:
          - cidr: 192.168.0.0/24
            resource_tags: {"Name": "k8s-{{ job_id }}-public"}
        internet_gateway: True
        route_tables:
          - subnets:
              - 192.168.0.0/24
            routes:
              - dest: 0.0.0.0/0
                gw: igw
      register: vpc

    # - debug: var=vpc
    # - debug: var=vpc.subnets[0].id

    - name: Jump host security group
      ec2_group:
        name: management
        description: "Management end-point SG {{ job_id }}"
        region: "{{ aws_region }}"
        vpc_id: "{{ vpc.vpc_id }}"
        rules:
          - proto: tcp
            from_port: 22
            to_port: 22
            cidr_ip: 0.0.0.0/0
        rules_egress:
          - proto: all
            cidr_ip: 0.0.0.0/0
      register: sg_management

    # - debug: var=sg_management

    - name: Read default security group
      ec2_group:
        name: default
        description: "Default SG k8s-vpc-{{ job_id }}"
        state: present
        vpc_id: "{{ vpc.vpc_id }}"
        region: "{{ aws_region }}"
      register: sg_default

    - name: Default security group
      ec2_group:
        name: default
        description: "Default SG k8s-vpc-{{ job_id }}"
        vpc_id: "{{ vpc.vpc_id }}"
        region: "{{ aws_region }}"
        rules:
          - proto: all
            group_id: "{{ sg_default.group_id }}"
          - proto: all
            group_id: "{{ sg_management.group_id }}"
        rules_egress:
          - proto: all
            cidr_ip: 0.0.0.0/0
      register: sg_default

    - name: Management host
      ec2:
        image: "{{ ec2_image }}"
        key_name: k8s
        instance_tags: {"Name": "k8s-mgmt-{{ job_id }}", "Cluster": "k8s-ansible-{{ job_id }}"}
        instance_type: m3.medium
        region: "{{ aws_region }}"
        vpc_subnet_id: "{{ vpc.subnets[0].id }}"
        group_id: "{{ sg_management.group_id }}"
        assign_public_ip: yes
        count_tag:
          Name: "k8s-mgmt-{{ job_id }}"
        exact_count: 1
        wait: true
      register: k8s_management

    - add_host: name='{{ k8s_management.tagged_instances[0].public_dns_name }}' groups=management ansible_ssh_user="{{ ssh_user }}"

    - name: Create gateway instance
      ec2:
        image: "{{ ec2_image }}"
        key_name: k8s
        instance_tags: {"Name": "k8s-gateway-{{ job_id }}", "Cluster": "k8s-ansible-{{ job_id }}"}
        instance_type: m3.medium
        region: "{{ aws_region }}"
        vpc_subnet_id: "{{ vpc.subnets[0].id }}"
        assign_public_ip: yes
        source_dest_check: no
        count_tag:
          Name: "k8s-gateway-{{ job_id }}"
        exact_count: 1
      register: k8s_gateway

    - name: Create master instance
      ec2:
        image: "{{ ec2_image }}"
        key_name: k8s
        instance_tags: {"Name": "k8s-master-{{ job_id }}", "Cluster": "k8s-ansible-{{ job_id }}"}
        instance_type: m3.large
        region: "{{ aws_region }}"
        vpc_subnet_id: "{{ vpc.subnets[0].id }}"
        assign_public_ip: yes        
        count_tag:
          Name: "k8s-master-{{ job_id }}"
        exact_count: 1
      register: k8s_master

    - name: Create nodes
      ec2:
        image: "{{ ec2_image }}"
        key_name: k8s
        instance_tags:
          Name: "k8s-node-{{ job_id }}"
          Cluster: "k8s-ansible-{{ job_id }}"
        instance_type: m3.medium
        region: "{{ aws_region }}"
        vpc_subnet_id: "{{ vpc.subnets[0].id }}"
        assign_public_ip: yes
        count_tag:
          Name: "k8s-node-{{ job_id }}"
        exact_count: 2
      register: k8s_nodes

    # ec2_vpc_route_table is only supported in ansible 2.0+
    # - name: Add a route to the public network
    #   ec2_vpc_route_table:
    #     tags:
    #       Name: "rt-k8s-{{ job_id }}-public"
    #     vpc_id: "{{ vpc.vpc_id }}"
    #     region: "{{ aws_region }}"
    #     routes:
    #       - subnets:
    #           - 192.168.0.0/24
    #         routes:
    #           - dest: 0.0.0.0/0
    #             gw: igw
    #           - dest: "{{ k8s_public_subnet }}"
    #             instance_id: "{{ k8s_gateway.tagged_instances[0].id }}"

    - name: Update the vpc
      ec2_vpc:
        state: present
        region: "{{ aws_region }}"
        cidr_block: 192.168.0.0/16
        resource_tags: {"Name": "k8s-vpc-{{ job_id }}", "Cluster": "k8s-ansible-{{ job_id }}"}
        vpc_id: "{{ vpc.vpc_id }}"
        subnets:
          - cidr: 192.168.0.0/24
            resource_tags: {"Name": "k8s-{{ job_id }}-public"}
        internet_gateway: True
        route_tables:
          - subnets:
              - 192.168.0.0/24
            routes:
              - dest: 0.0.0.0/0
                gw: igw
              - dest: "{{ k8s_public_subnet }}"
                gw: "{{ k8s_gateway.tagged_instances[0].id }}"

    - name: Store the status of the cluster
      template: src=status.j2 dest="{{ status_dir }}/cluster.status"

    - name: Create inventory file
      template: src=inventory.j2 dest="{{ status_dir }}/inventory.cluster"

    - name: VPC status
      template: src=vpc.j2 dest="{{ status_dir }}/vpc.status"

    - wait_for: host="{{ k8s_management.tagged_instances[0].public_dns_name }}" port=22

  vars:
    job_id: 01
    aws_region: us-west-1
    ec2_image: ami-8ebba3cb

# Tasks that require root priviledges
- hosts: management
  sudo: yes
  tasks:
    - name: Update apt cache
      apt: update_cache=yes cache_valid_time=3600

    - name: Python requirements
      apt: name="{{ item }}" state=present
      with_items:
        - python-dev
        - python-pip
        - python-netaddr
        - python-markupsafe

    - name: Requires ansible
      pip: name=ansible state=present

    - name: Requires git
      apt: name=git state=present

# tasks that can run under regular user
- hosts: management
  tasks:
    - name: tmp directory
      file: path="{{ path_tmp }}" state=directory

    - name: Download kubernetes release
      get_url: url=https://github.com/kubernetes/kubernetes/releases/download/v1.1.1/kubernetes.tar.gz dest="{{ path_tmp }}"
      # creates: "{{ path_tmp }}/kubernetes.tar.gz"

    - name: Unpack kubernetes tarball
      unarchive:
      args:
        src: "{{ path_tmp }}/kubernetes.tar.gz"
        dest: "{{ path_tmp }}"
        copy: no
        creates: "{{ path_tmp }}/kubernetes/server/kubernetes-server-linux-amd64.tar.gz"

    - name: Unpack linux binaries
      unarchive:
      args:
        src: "{{ path_tmp }}/kubernetes/server/kubernetes-server-linux-amd64.tar.gz"
        dest: "{{ path_tmp }}"
        copy: no
        creates: "{{ path_tmp }}/kubernetes/server/bin/kube-apiserver"

    - name: Ansible configuration file
      copy: src=ansible.cfg dest="~/.ansible.cfg"

    - name: Code directory
      file: path="{{ path_src }}" state=directory

    - name: Fetch provisioning scripts
      git: repo=https://github.com/pedro-r-marques/contrib.git dest="{{ path_src }}/contrib" version=opencontrail update=no

    - name: Copy inventory file
      copy: src="{{ status_dir }}/inventory.cluster" dest="{{ path_src }}/contrib/ansible/inventory"

    - name: Copy playbooks
      copy: src="{{ item }}" dest="{{ path_src }}/contrib/ansible"
      with_items:
        - resolution.yml
        - validate.yml
        - examples.yml

    - name: Enable opencontrail in group_vars
      lineinfile:
        dest: "{{ path_src }}/contrib/ansible/group_vars/all.yml"
        regexp: "^networking: "
        line: "networking: opencontrail"

    - name: Configure address resolution
      command: ansible-playbook -i inventory resolution.yml
      args:
        chdir: "{{ path_src }}/contrib/ansible"

    # - name: Provision cluster
    #   command: "./setup.sh"
    #   args:
    #     chdir: "{{ path_src }}/contrib/ansible"
